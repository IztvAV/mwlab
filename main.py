import os
import random
from pathlib import Path

import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor
from torch.nn import MSELoss, L1Loss

from common import check_metrics
from filters.mwfilter_optim.base import FastMN2toSParamCalculation
from mwlab.nn.scalers import MinMaxScaler, StdScaler
from mwlab import TouchstoneDataset, TouchstoneLDataModule, TouchstoneDatasetAnalyzer

from filters import CMTheoreticalDatasetGenerator, CMTheoreticalDatasetGeneratorSamplers, SamplerTypes, MWFilter, CouplingMatrix
from filters.codecs import MWFilterTouchstoneCodec
from filters.mwfilter_lightning import MWFilterBaseLModule, MWFilterBaseLMWithMetrics

from filters.datasets.theoretical_dataset_generator import CMShifts, PSShift

import matplotlib.pyplot as plt
import lightning as L

from torch import nn
import models
import torch
from filters.mwfilter_optim.bfgs import optimize_cm
from losses import CustomLosses
import configs
import common

import sklearn
import numpy as np
from sklearn.multioutput import MultiOutputRegressor
import joblib
from tqdm import tqdm
import sensivity
from torchvision import models as t_models

from mwlab.transforms import TComposite
from mwlab.transforms.s_transforms import S_Crop, S_Resample

torch.set_float32_matmul_precision("medium")

# ---------- 2. –°–±–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ----------
def collect_nn_predictions(model, dataloader):
    model.eval()
    preds, targets = [], []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="NN prediction"):
            x, y = batch
            if hasattr(model, 'to'):  # –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —ç—Ç–æ LightningModule
                x = x.to(model.device)
            y_pred = model(x).cpu()
            preds.append(y_pred.numpy())
            targets.append(y.numpy())

    return np.concatenate(preds, axis=0), np.concatenate(targets, axis=0)


def fit_minmax_scalers(preds, targets):
    pred_scaler = sklearn.preprocessing.MinMaxScaler()
    target_scaler = sklearn.preprocessing.MinMaxScaler()
    pred_scaled = pred_scaler.fit_transform(preds)
    target_scaled = target_scaler.fit_transform(targets)
    return pred_scaler, target_scaler, pred_scaled, target_scaled


def train_boosting_model(X_train_scaled, y_train_scaled, params=None):
    if params is None:
        params = dict(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=4,
            random_state=42,
            verbose=1
        )

    base_model = GradientBoostingRegressor(**params)
    model = MultiOutputRegressor(base_model)

    print("‚û°Ô∏è –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model.fit(X_train_scaled, y_train_scaled)

    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –í—ã—á–∏—Å–ª—è—é –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ...\n")

    y_pred = model.predict(X_train_scaled)

    mse = sklearn.metrics.mean_squared_error(y_train_scaled, y_pred)
    mae = sklearn.metrics.mean_absolute_error(y_train_scaled, y_pred)
    r2 = sklearn.metrics.r2_score(y_train_scaled, y_pred)


    print("\nüìà M–µ—Ç—Ä–∏–∫–∏:")
    print(f"  MSE = {mse:.4f}")
    print(f"  R¬≤  = {r2:.4f}")
    print(f"  MAE = {mae:.4f}")

    return model


def save_boosting_pipeline(path: str | Path, model, pred_scaler, target_scaler):
    path = Path(path)
    path.mkdir(exist_ok=True, parents=True)
    joblib.dump(model, path / "gb_model.joblib")
    joblib.dump(pred_scaler, path / "scaler_pred.joblib")
    joblib.dump(target_scaler, path / "scaler_target.joblib")


def train_corrector(lightning_model, datamodule, output_dir="./gb_corrector"):
    datamodule.setup("fit")
    train_loader = datamodule.train_dataloader()

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
    print("Get AI-prediction")
    preds, targets = collect_nn_predictions(lightning_model, train_loader)

    # –ù–æ—Ä–º–∏—Ä—É–µ–º
    print("Normalize data")
    pred_scaler, target_scaler, X_scaled, y_scaled = fit_minmax_scalers(preds, targets)

    # –û–±—É—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥
    print("Start train ml-model")
    model = train_boosting_model(X_scaled, y_scaled)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Å–∫–µ–π–ª–µ—Ä—ã
    save_boosting_pipeline(output_dir, model, pred_scaler, target_scaler)

    print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å –∏ —Å–∫–µ–π–ª–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")


def predict_with_corrector(nn_preds: np.ndarray, corrector_dir: str | Path) -> np.ndarray:
    """
    –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–π ML-–º–æ–¥–µ–ª–∏ –∏ —Å–∫–µ–π–ª–µ—Ä–æ–≤.

    :param nn_preds: numpy-–º–∞—Å—Å–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (—Ä–∞–∑–º–µ—Ä [N, D])
    :param corrector_dir: –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –º–æ–¥–µ–ª—å—é –∏ —Å–∫–µ–π–ª–µ—Ä–∞–º–∏
    :return: –æ—Ç–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≤ –∏—Å—Ö–æ–¥–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ)
    """
    corrector_dir = Path(corrector_dir)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Å–∫–µ–π–ª–µ—Ä–æ–≤
    model = joblib.load(corrector_dir / "gb_model.joblib")
    pred_scaler = joblib.load(corrector_dir / "scaler_pred.joblib")
    target_scaler = joblib.load(corrector_dir / "scaler_target.joblib")

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    nn_preds_scaled = pred_scaler.transform(nn_preds)

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    corrected_scaled = model.predict(nn_preds_scaled)

    # –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π –º–∞—Å—à—Ç–∞–±
    corrected = target_scaler.inverse_transform(corrected_scaled)
    return corrected

def fine_tune_model(inference_model: nn.Module, target_input: MWFilter, meta: dict, device='cuda', epochs=10, lr=1e-4):
    """
    model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    generate_nearby_dataset: —Ñ—É–Ω–∫—Ü–∏—è, —Å–æ–∑–¥–∞—é—â–∞—è –¥–∞—Ç–∞—Å–µ—Ç –æ–∫–æ–ª–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    target_input: –≤—Ö–æ–¥, –≤–±–ª–∏–∑–∏ –∫–æ—Ç–æ—Ä–æ–≥–æ —Ö–æ—Ç–∏–º –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å
    """
    # üîÅ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç –æ–∫–æ–ª–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    orig_fil = common.create_origin_filter(configs.ENV_ORIGIN_DATA_PATH)
    for i in range(5):
        print(f"Iteration: {i}")
        pred_prms = inference_model.predict_x(target_input)
        pred_fil = inference_model.create_filter_from_prediction(orig_fil, pred_prms, meta)
        sampler_configs = {
            "pss_origin": PSShift(phi11=1e-12, phi21=1e-12, theta11=1e-12, theta21=1e-12),
            "pss_shifts_delta": PSShift(phi11=1e-12, phi21=1e-12, theta11=1e-12, theta21=1e-12),
            "cm_shifts_delta": CMShifts(self_coupling=0.1, mainline_coupling=0.1, cross_coupling=0.0001),
            "samplers_size": 1000,
        }
        samplers = CMTheoreticalDatasetGeneratorSamplers.create_samplers(pred_fil,
                                                                                    samplers_type=SamplerTypes.SAMPLER_SOBOL(
                                                                                        one_param=False),
                                                                                        **sampler_configs
                                                                                    )
        ds_gen = CMTheoreticalDatasetGenerator(
            path_to_save_dataset=os.path.join(configs.ENV_TUNE_DATASET_PATH, samplers.cms.type.name, f"{len(samplers.cms)}"),
            backend_type='ram',
            orig_filter=pred_fil,
            filename="Dataset",
            rewrite=True
        )
        ds_gen.generate(samplers)

        ds = TouchstoneDataset(source=ds_gen.backend, in_memory=True)
        # common.plot_distribution(ds, num_params=len(ds_gen.origin_filter.coupling_matrix.links))
        # plt.show()
        codec = MWFilterTouchstoneCodec.from_dataset(ds=ds,
                                                     keys_for_analysis=[f"m_{r}_{c}" for r, c in orig_fil.coupling_matrix.links])
        dm = TouchstoneLDataModule(
            source=ds_gen.backend,  # –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
            codec=codec,  # –ö–æ–¥–µ–∫ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è TouchstoneData ‚Üí (x, y)
            batch_size=configs.BATCH_SIZE,  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            val_ratio=0,  # –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞
            test_ratio=0,  # –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞
            cache_size=None,
            scaler_in=MinMaxScaler(dim=(0, 2), feature_range=(0, 1)),  # –°–∫–µ–π–ª–µ—Ä –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            scaler_out=MinMaxScaler(dim=0, feature_range=(-0.5, 0.5)),  # –°–∫–µ–π–ª–µ—Ä –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            swap_xy=True,
            num_workers=0,
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞–∑–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:
            base_ds_kwargs={
                "in_memory": True
            }
        )
        dm.setup("fit")
        dataloader = dm.train_dataloader()
        inference_model = inference_model.to(device)
        inference_model.train()

        # üîß –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        trainer = L.Trainer(
            deterministic=True,
            max_epochs=epochs,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            accelerator="auto",  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (CPU/GPU)
            log_every_n_steps=100,  # –ß–∞—Å—Ç–æ—Ç–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è
        )
        trainer.fit(inference_model, dm)

    return inference_model


def main_ae():
    sampler_configs = {
        "pss_origin": PSShift(phi11=0, phi21=0, theta11=0, theta21=0),
        "pss_shifts_delta": PSShift(phi11=0.00000001, phi21=0.00000001, theta11=0.00000001, theta21=0.00000001),
        "cm_shifts_delta": CMShifts(self_coupling=1.5, mainline_coupling=0.1, cross_coupling=5e-3,
                                    parasitic_coupling=5e-3),
        "samplers_size": configs.BASE_DATASET_SIZE,
    }
    work_model = common.AEWorkModel(configs.ENV_DATASET_PATH, sampler_configs, SamplerTypes.SAMPLER_SOBOL)
    # common.plot_distribution(work_model.ds, num_params=len(work_model.ds_gen.origin_filter.coupling_matrix.links))
    # plt.show()

    codec = MWFilterTouchstoneCodec.from_dataset(ds=work_model.ds,
                                                 keys_for_analysis=[f"m_{r}_{c}" for r, c in
                                                                    work_model.orig_filter.coupling_matrix.links])
    codec.y_channels = ['S1_1.db', 'S1_2.db', 'S2_1.db', 'S2_2.db']
    codec = codec
    work_model.setup(
        # model_name="cae",
        # model_cfg={"in_ch":len(codec.y_channels), "z_dim":work_model.orig_filter.order*6},
        model_name="imp_cae",
        model_cfg={"in_ch":len(codec.y_channels), "z_dim":work_model.orig_filter.order*6},
        # model_name="mlp",
        # model_cfg={},
        dm_codec=codec
    )

    lit_model = work_model.train(
        optimizer_cfg={"name": "AdamW", "lr": 0.0005371, "weight_decay": 1e-2},
        scheduler_cfg={"name": "StepLR", "step_size": 24, "gamma": 0.01},
        loss_fn=L1Loss()
    )

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    inference_model = work_model.inference(lit_model.trainer.checkpoint_callback.best_model_path)
    # inference_model = work_model.inference("saved_models\\EAMU4-KuIMUXT3-BPFC1\\best-epoch=24-train_loss=0.00163-val_loss=0.00146-val_r2=0.99352-val_mse=0.00002-val_mae=0.00146-batch_size=32-base_dataset_size=1000000-sampler=SamplerTypes.SAMPLER_SOBOL.ckpt")

    check_metrics(trainer=work_model.trainer, model=inference_model, dm=work_model.dm)

    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    orig_fil, pred_fil = inference_model.predict(work_model.dm, idx=0)
    inference_model.plot_origin_vs_prediction(orig_fil, pred_fil)
    plt.figure()
    plt.plot(orig_fil.f, np.real(orig_fil.s[:, 0, 0]), orig_fil.f, np.real(pred_fil.s[:, 0, 0]),
             orig_fil.f, np.imag(orig_fil.s[:, 0, 0]), orig_fil.f, np.imag(pred_fil.s[:, 0, 0]))
    plt.legend(["real S11 orig", "real S11 pred", "imag S11 orig", "imag S11 pred"])
    plt.title("S11")

    plt.figure()
    plt.plot(orig_fil.f, np.real(orig_fil.s[:, 1, 0]), orig_fil.f, np.real(pred_fil.s[:, 1, 0]),
             orig_fil.f, np.imag(orig_fil.s[:, 1, 0]), orig_fil.f, np.imag(pred_fil.s[:, 1, 0]))
    plt.legend(["real S21 orig", "real S21 pred", "imag S21 orig", "imag S21 pred"])
    plt.title("S21")


def main_extract_matrix():
    sampler_configs = {
        "pss_origin": PSShift(phi11=0.547, phi21=-1.0, theta11=0.01685, theta21=0.017),
        "pss_shifts_delta": PSShift(phi11=0.02, phi21=0.02, theta11=0.005, theta21=0.005),
        # "cm_shifts_delta": CMShifts(self_coupling=1.8, mainline_coupling=0.3, cross_coupling=9e-2, parasitic_coupling=5e-3), # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞
        "cm_shifts_delta": CMShifts(self_coupling=0.5, mainline_coupling=0.1, cross_coupling=5e-3,
                                    parasitic_coupling=5e-3),
        "samplers_size": configs.BASE_DATASET_SIZE,
    }
    work_model = common.WorkModel(configs.ENV_DATASET_PATH, sampler_configs, SamplerTypes.SAMPLER_STD)
    # sensivity.run(work_model.orig_filter)
    # common.plot_distribution(work_model.ds, num_params=len(work_model.ds_gen.origin_filter.coupling_matrix.links))
    # plt.show()

    codec = MWFilterTouchstoneCodec.from_dataset(ds=work_model.ds,
                                                 keys_for_analysis=[f"m_{r}_{c}" for r, c in work_model.orig_filter.coupling_matrix.links])
    codec = codec
    # work_model_inference = common.WorkModel(configs.ENV_DATASET_PATH, 1000, SamplerTypes.SAMPLER_SOBOL)
    # work_model_inference.setup(
    #     model_name="resnet_with_correction",
    #     model_cfg={"in_channels": len(codec.y_channels), "out_channels": len(codec.x_keys)},
    #     dm_codec=codec
    # )
    # work_model_inference.inference("saved_models\\EAMU4T1-BPFC2\\best-epoch=25-train_loss=0.03897-val_loss=0.04207-val_r2=0.92781-val_mse=0.00573-val_mae=0.03634-batch_size=32-base_dataset_size=500000-sampler=SamplerTypes.SAMPLER_SOBOL.ckpt")
    # work_model.setup(
    #     model_name="resnet_with_wide_correction",
    #     model_cfg={"main_model": work_model_inference.model,
    #                "in_channels": len(codec.y_channels), "out_channels": len(codec.x_keys)},
    #     dm_codec=codec
    # )
    # work_model.inference("saved_models\\EAMU4T1-BPFC2\\best-epoch=25-train_loss=0.02530-val_loss=0.02793-val_r2=0.96251-val_mse=0.00296-val_mae=0.02496-batch_size=32-base_dataset_size=1000000-sampler=SamplerTypes.SAMPLER_SOBOL.ckpt")
    #
    # work_model.setup(
    #     model_name="resnet_with_correction",
    #     model_cfg={"in_channels": len(codec.y_channels), "out_channels": len(codec.x_keys)},
    #     dm_codec=codec
    # )
    work_model.setup(
        model_name="cae",
        model_cfg={"in_channels": len(codec.y_channels), "out_channels": len(codec.x_keys)},
        dm_codec=codec
    )

    lit_model = work_model.train(
        # optimizer_cfg={"name": "AdamW", "lr": 0.0009400000000000001, "weight_decay": 1e-5},
        # scheduler_cfg={"name": "StepLR", "step_size": 28, "gamma": 0.09},
        optimizer_cfg={"name": "AdamW", "lr": 0.0005371, "weight_decay": 1e-5},
        scheduler_cfg={"name": "StepLR", "step_size": 24, "gamma": 0.01},
        loss_fn=CustomLosses("mse_with_l1", weight_decay=1, weights=None)
        )

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    # checkpoint_path="saved_models\\SCYA501-KuIMUXT5-BPFC3\\best-epoch=12-val_loss=0.01266-train_loss=0.01224.ckpt",
    # checkpoint_path="saved_models\\EAMU4-KuIMUXT3-BPFC1\\best-epoch=49-train_loss=0.03379-val_loss=0.03352-val_r2=0.84208-val_acc=0.25946-val_mae=0.05526-batch_size=32-dataset_size=100000.ckpt",
    # "saved_models/ERV-KuIMUXT1-BPFC1/best-epoch=22-train_loss=0.04863-val_loss=0.05762-val_r2=0.82785-val_mse=0.01366-val_mae=0.04395-batch_size=32-base_dataset_size=100000-sampler=SamplerTypes.SAMPLER_SOBOL.ckpt"
    # inference_model = work_model.inference("saved_models\\EAMU4-KuIMUXT3-BPFC1\\best-epoch=29-train_loss=0.04166-val_loss=0.04450-val_r2=0.92560-val_mse=0.00588-val_mae=0.03862-batch_size=32-base_dataset_size=1500000-sampler=SamplerTypes.SAMPLER_SOBOL.ckpt")
    # inference_model = work_model.inference("saved_models\\SCYA501-KuIMUXT5-BPFC3\\best-epoch=29-train_loss=0.03546-val_loss=0.03841-val_r2=0.94190-val_mse=0.00459-val_mae=0.03381-batch_size=32-base_dataset_size=500000-sampler=SamplerTypes.SAMPLER_SOBOL.ckpt")
    # inference_model = work_model.inference("saved_models\\EAMU4T1-BPFC2\\best-epoch=34-train_loss=0.02388-val_loss=0.02641-val_r2=0.96637-val_mse=0.00265-val_mae=0.02376-batch_size=32-base_dataset_size=600000-sampler=SamplerTypes.SAMPLER_SOBOL.ckpt")
    # inference_model = work_model.inference("saved_models\\EAMU4T1-BPFC2\\best-epoch=25-train_loss=0.02530-val_loss=0.02793-val_r2=0.96251-val_mse=0.00296-val_mae=0.02496-batch_size=32-base_dataset_size=1000000-sampler=SamplerTypes.SAMPLER_SOBOL.ckpt")
    inference_model = work_model.inference(lit_model.trainer.checkpoint_callback.best_model_path)

    # work_model_widenet = common.WorkModel(configs.ENV_DATASET_PATH, configs.BASE_DATASET_SIZE+100000, SamplerTypes.SAMPLER_SOBOL)
    # work_model_widenet.setup(
    #     model_name="resnet_with_wide_correction",
    #     model_cfg={"main_model": inference_model.model,
    #                "in_channels": len(codec.y_channels), "out_channels": len(codec.x_keys)},
    #     dm_codec=codec
    # )
    #
    # lit_model_widenet = work_model_widenet.train(
    #     optimizer_cfg={"name": "AdamW", "lr": 0.0009400000000000001, "weight_decay": 1e-5},
    #     scheduler_cfg={"name": "StepLR", "step_size": 28, "gamma": 0.09},
    #     # optimizer_cfg={"name": "AdamW", "lr": 0.0005371, "weight_decay": 1e-5},
    #     # scheduler_cfg={"name": "StepLR", "step_size": 24, "gamma": 0.01},
    #     loss_fn=CustomLosses("mse_with_l1", weight_decay=1, weights=None)
    # )
    # inference_model = work_model_widenet.inference(lit_model_widenet.trainer.checkpoint_callback.best_model_path)

    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    # orig_fil, pred_fil = inference_model.predict(work_model.dm, idx=0)
    # inference_model.plot_origin_vs_prediction(orig_fil, pred_fil)
    # optim_matrix = optimize_cm(pred_fil, orig_fil)
    # error_matrix_pred = CouplingMatrix.error_matrix(orig_fil.coupling_matrix, pred_fil.coupling_matrix)
    # error_matrix_optim = CouplingMatrix.error_matrix(orig_fil.coupling_matrix, optim_matrix)
    #
    # error_matrix_optim.plot_matrix(title="Optimized de-tuned matrix errors", cmap="YlOrBr")
    # optim_matrix.plot_matrix(title="Optimized de-tuned matrix")
    # error_matrix_pred.plot_matrix(title="Predict de-tuned matrix errors", cmap="YlOrBr")
    # pred_fil.coupling_matrix.plot_matrix(title="Predict de-tuned matrix")
    # orig_fil.coupling_matrix.plot_matrix(title="Origin de-tuned matrix")


    #
    # optim_matrix = optimize_cm(pred_fil, orig_fil)
    # error_matrix_pred = CouplingMatrix.error_matrix(orig_fil.coupling_matrix, pred_fil.coupling_matrix)
    # error_matrix_optim = CouplingMatrix.error_matrix(orig_fil.coupling_matrix, optim_matrix)
    # error_matrix_optim.plot_matrix(title="Optimized tuned matrix errors for inverted model", cmap="YlOrBr")
    # error_matrix_pred.plot_matrix(title="Predict tuned matrix errors for inverted model", cmap="YlOrBr")
    # orig_fil.coupling_matrix.plot_matrix(title="Origin tuned matrix for inverted model")
    # pred_fil.coupling_matrix.plot_matrix(title="Predict tuned matrix for inverted model")
    # optim_matrix.plot_matrix(title="Optimized tuned matrix for inverted model")


    tds = TouchstoneDataset(f"filters/FilterData/{configs.FILTER_NAME}/modeling",
                            s_tf=TComposite(
                                [S_Crop(f_start=work_model.orig_filter.f[0], f_stop=work_model.orig_filter.f[-1]),
                                 S_Resample(len(work_model.orig_filter.f))]))
    for i in range(1):
        # i = random.randint(0, len(tds))
        orig_fil = tds[2][1]
        # start_time = time.time()
        pred_prms = inference_model.predict_x(orig_fil)
        # stop_time = time.time()
        # print(f"Predict time: {stop_time - start_time:.3f} sec")
        print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {pred_prms}")
        pred_fil = inference_model.create_filter_from_prediction(orig_fil, pred_prms, work_model.meta)
        inference_model.plot_origin_vs_prediction(orig_fil, pred_fil)
        optim_matrix = optimize_cm(pred_fil, orig_fil)
        # optim_matrix.plot_matrix(title="AI-extracted matrix (Res3)")
        # cst_matrix = CouplingMatrix.from_file("filters/FilterData/EAMU4T1-BPFC2/measure/AMU4_T1_Ch2_Measur_Res3_var_extr_matrix.txt")
        # cst_matrix.plot_matrix(title="CST-extracted matrix (Res3)")
        print(f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {optim_matrix.factors}")

        # plt.figure()
        # plt.plot(pred_fil.f_norm, orig_fil.s_db[:, 0, 0], label='S11 origin')
        # plt.plot(pred_fil.f_norm, orig_fil.s_db[:, 1, 0], label='S21 origin')
        # plt.plot(pred_fil.f_norm, orig_fil.s_db[:, 1, 1], label='S22 origin')


    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä
    # orig_fil = work_model.ds_gen.origin_filter
    # pred_prms = inference_model.predict_x(orig_fil)
    # pred_fil = inference_model.create_filter_from_prediction(orig_fil, pred_prms, work_model.meta)
    # inference_model.plot_origin_vs_prediction(orig_fil, pred_fil)
    # optim_matrix = optimize_cm(pred_fil, orig_fil)
    # pred_fil.coupling_matrix.plot_matrix(title="Predict tuned matrix")
    # optim_matrix.plot_matrix(title="Optimized tuned matrix")
    # error_matrix_pred = CouplingMatrix.error_matrix(orig_fil.coupling_matrix, pred_fil.coupling_matrix)
    # error_matrix_optim = CouplingMatrix.error_matrix(orig_fil.coupling_matrix, optim_matrix)
    # error_matrix_optim.plot_matrix(title="Optimized tuned matrix errors")
    # error_matrix_pred.plot_matrix(title="Predict tuned matrix errors")
    # orig_fil.coupling_matrix.plot_matrix(title="Origin tuned matrix")
    plt.show()


def plot_pl_csv_wide(csv_path, outdir="artifacts_opt3"):
    os.makedirs(outdir, exist_ok=True)
    df = pd.read_csv(csv_path)

    # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
    by_epoch = df.groupby("epoch").last(numeric_only=True)

    epochs = by_epoch.index.values
    train_loss = by_epoch["train_loss_epoch"]
    val_loss = by_epoch["val_loss"]
    val_r2 = by_epoch["val_r2"]

    fig, ax1 = plt.subplots()

    # –ü–µ—Ä–≤–∞—è –æ—Å—å ‚Äî –ø–æ—Ç–µ—Ä–∏
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("Loss", color="tab:blue")
    ax1.plot(epochs, train_loss, label="Train Loss", color="tab:blue", linestyle="--")
    ax1.plot(epochs, val_loss, label="Val Loss", color="tab:blue")
    ax1.tick_params(axis='y', labelcolor="tab:blue")

    # –í—Ç–æ—Ä–∞—è –æ—Å—å ‚Äî R¬≤
    ax2 = ax1.twinx()
    ax2.set_ylabel("R¬≤", color="tab:green")
    ax2.plot(epochs, val_r2, label="Val R¬≤", color="tab:green", linestyle=":")
    ax2.tick_params(axis='y', labelcolor="tab:green")

    # –õ–µ–≥–µ–Ω–¥–∞
    lines_1, labels_1 = ax1.get_legend_handles_labels()
    lines_2, labels_2 = ax2.get_legend_handles_labels()
    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc="center right", frameon=True, fontsize=9)

    # plt.title("–ú–µ—Ç—Ä–∏–∫–∏")
    fig.tight_layout()

    # plt.savefig(os.path.join(outdir, "loss_r2_curves.png"), dpi=200)
    # plt.savefig(os.path.join(outdir, "loss_r2_curves.svg"))
    plt.show()


if __name__ == "__main__":
    # csv_path = "lightning_logs/cae_csv/version_71/metrics.csv"
    # plot_pl_csv_wide(csv_path)
    main_ae()
    plt.show()
